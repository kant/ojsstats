{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, re, os, gc\n",
      "import difflib, random\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv, json\n",
      "from unidecode import unidecode\n",
      "\n",
      "import sqlite3 as lite\n",
      "\n",
      "from datetime import date, timedelta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = pd.read_csv('data/dates.txt', sep=\"\\t\")\n",
      "record_dates.columns = ['record_id', 'archive_id','dcdate', 'source']\n",
      "record_dates.set_index('archive_id', inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "archive_settings  = pd.read_csv('data/archive_settings.csv', encoding='utf8', escapechar='\\\\')\n",
      "archive_settings.columns = ['archive_id','setting_name', 'setting_value', 'type']\n",
      "archive_settings.set_index('archive_id', inplace=True)\n",
      "lastIndexedDate = archive_settings[archive_settings.setting_name == 'lastIndexedDate'][['setting_value']]\n",
      "lastIndexedDate.columns=['lastIndexedDate']                                                                        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(lastIndexedDate, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove stuff we haven't harvested in the last 30 days\n",
      "record_dates = record_dates[record_dates.lastIndexedDate >= (date.today() - timedelta(days=30)).strftime(\"%Y-%m-%d\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "litecon = lite.connect('data/ojs_oai.db')\n",
      "countries = pd.read_sql(\"select archive_id, country, region_id, region_name from locales\", litecon, index_col='archive_id')\n",
      "countries.index = countries.index.astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(countries, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "installations = pd.read_sql(\"select archive_id, repository_identifier, ip from journals\", litecon, index_col='archive_id')\n",
      "installations['install_id'] = installations.apply(lambda row: \"%s_%s\" % (row['repository_identifier'], row['ip']), axis=1)\n",
      "del installations['repository_identifier']\n",
      "del installations['ip']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(installations, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.reset_index().set_index('record_id')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# archive_settings cleanup to make Alex happy\n",
      "try: \n",
      "    del archive_settings\n",
      "    del lastIndexedDate\n",
      "    del countries\n",
      "    del installations\n",
      "    cleanedup = gc.collect()\n",
      "except:\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "year_regex = re.compile('.*(?:[^\\d\\-]|^)((?:1\\d|20)\\d{2})(?:\\)|\\;|$).*')\n",
      "\n",
      "# pull the year out of the ( ) if it is present\n",
      "def find_best_year(y, s):\n",
      "    try:\n",
      "        y = int(y)\n",
      "        if not (y <= date.today().year and y > 1000):\n",
      "            y = None  # this means an invalid year in dcdate\n",
      "    except:\n",
      "        y = None\n",
      "\n",
      "    try: \n",
      "        r=year_regex.match(s)\n",
      "        year_in_source = int(r.group(1))\n",
      "        if year_in_source <= date.today().year and year_in_source > 1000:\n",
      "            y = year_in_source\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    return y\n",
      "# We're doing chained assignment somewhere here, but its not a problem, so turn off warning\n",
      "pd.options.mode.chained_assignment = None \n",
      "\n",
      "#\n",
      "# a simple attempt at fixing wrong publication years by pulling a year from the source\n",
      "#\n",
      "record_dates['year'] = record_dates.dcdate.astype(str).map(lambda x: x[0:4])\n",
      "record_dates['year'] = record_dates.apply(lambda row: find_best_year(row['year'], row['source']), axis=1)\n",
      "\n",
      "# cleanup the years column\n",
      "record_dates = record_dates[record_dates.year > 0] # remove things that still have no date\n",
      "\n",
      "# and cast column to int to remove .0 from the end when it gets turned to str later\n",
      "record_dates['year'] = record_dates.year.astype(int)\n",
      "\n",
      "\n",
      "# update dates that are likely to be in the islamic calendar\n",
      "record_dates['year'] = record_dates.year.map(lambda x: x if ((x <= date.today().year) and (x >= 1900 - 622)) else x + 622)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# grab only those journals that have at least x articles in that year\n",
      "last_year = date.today().year - 1\n",
      "\n",
      "def filter_by_num_articles(n):\n",
      "    grouped=record_dates.reset_index().groupby(['year', 'archive_id'])\n",
      "\n",
      "    filtered = grouped.filter(lambda x: len(x) >= n)\n",
      "\n",
      "    journals_data = filtered.ix[:,['archive_id', 'year']]\n",
      "    journals_data.to_csv('data/journals_with_%s_articles_any_year.csv' % n)\n",
      "\n",
      "    journals_data = filtered[filtered['year'] == last_year].ix[:,['archive_id','year']]\n",
      "    journals_data.to_csv('data/journals_with_%s_articles_%s.csv' % (n, last_year))\n",
      "\n",
      "    return filtered, journals_data\n",
      "\n",
      "filtered, journals_data = filter_by_num_articles(10)\n",
      "print len(filtered)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "countdata = {}\n",
      "articles_per_journal = {}\n",
      "# record_dates = record_dates[record_dates.year >= 1990]\n",
      "last_year = date.today().year - 1\n",
      "for year in map(int, sorted(record_dates.year.unique())):\n",
      "    if year < 1990 or year > last_year:\n",
      "        continue\n",
      "        \n",
      "    count_journals = len(filtered[filtered[\"year\"]==year].archive_id.unique())\n",
      "    count_articles = len(filtered[filtered[\"year\"]==year])\n",
      "    count_hosts = len(filtered[filtered[\"year\"]==year].install_id.unique())\n",
      "\n",
      "    # num_journals, num_articles, num_hosts\n",
      "    countdata[year] = [count_journals, count_articles, count_hosts]\n",
      "\n",
      "    # get number of articles per journal per year\n",
      "    # count, mean, std, min, 25%, 50%, 75%, max\n",
      "    articles_per_journal[year] = record_dates[record_dates['year']==year].groupby('archive_id').size().describe().tolist()\n",
      "\n",
      "\n",
      "# write the journal/article/hosts count\n",
      "f = open('data/ojs_counts.csv', 'wb')\n",
      "csvWriter = csv.writer(f)\n",
      "csvWriter.writerow(['year', 'journals', 'articles', 'hosts'])\n",
      "for year, data in countdata.iteritems():\n",
      "    csvWriter.writerow([year] + data)\n",
      "f.close()\n",
      "\n",
      "# write the articles per year counts\n",
      "f = open('data/articles_per_journal.csv', 'wb')\n",
      "csvWriter = csv.writer(f)\n",
      "csvWriter.writerow(['year', 'count', 'mean', 'std', 'min', 'p25', 'p50', 'p75', 'max'])\n",
      "for year, data in articles_per_journal.iteritems():\n",
      "    csvWriter.writerow([year] + data)\n",
      "f.close()\n",
      "\n",
      "# write the number of journals per country\n",
      "filtered.groupby(['year', 'country', 'region_id', 'region_name']).archive_id.unique().apply(len).to_csv('data/journals_per_country.csv', header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
