{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, re, os, gc\n",
      "import difflib, random\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv, json\n",
      "from unidecode import unidecode\n",
      "\n",
      "import sqlite3 as lite\n",
      "\n",
      "from datetime import date, timedelta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = pd.read_csv('data/dates.txt', sep=\"\\t\")\n",
      "record_dates.columns = ['record_id', 'archive_id','dcdate', 'source']\n",
      "record_dates.set_index('archive_id', inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "archive_settings  = pd.read_csv('data/archive_settings.csv', encoding='utf8', escapechar='\\\\')\n",
      "archive_settings.columns = ['archive_id','setting_name', 'setting_value', 'type']\n",
      "archive_settings.set_index('archive_id', inplace=True)\n",
      "lastIndexedDate = archive_settings[archive_settings.setting_name == 'lastIndexedDate'][['setting_value']]\n",
      "lastIndexedDate.columns=['lastIndexedDate']\n",
      "recordCount = archive_settings[archive_settings.setting_name == 'recordCount'][['setting_value']]\n",
      "recordCount.columns=['recordCount']\n",
      "\n",
      "def find_journal_url(oai_url):\n",
      "\tif oai_url.find(\"page=oai\") > 0:\n",
      "\t\tjournal_url = re.sub('page=oai.*', '', oai_url)\n",
      "\telse:\n",
      "\t\tjournal_url = re.sub('/oai.*?$', '', oai_url)\n",
      "\n",
      "\treturn journal_url\n",
      "\n",
      "harvesterUrl = archive_settings[archive_settings.setting_name == 'harvesterUrl'][['setting_value']]\n",
      "harvesterUrl.columns=['url']\n",
      "harvesterUrl['url'] = harvesterUrl.url.apply(find_journal_url)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(lastIndexedDate, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(recordCount, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(harvesterUrl, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove stuff we haven't harvested in the last 30 days\n",
      "record_dates = record_dates[record_dates.lastIndexedDate >= (date.today() - timedelta(days=30)).strftime(\"%Y-%m-%d\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "litecon = lite.connect('data/ojs_oai.db')\n",
      "countries = pd.read_sql(\"select archive_id, country, region_id, region_name from locales\", litecon, index_col='archive_id')\n",
      "countries.index = countries.index.astype(int)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(countries, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "installations = pd.read_sql(\"select archive_id, repository_identifier, ip, setName as journal_title, contact from journals\", litecon, index_col='archive_id')\n",
      "installations['install_id'] = installations.apply(lambda row: \"%s_%s\" % (row['repository_identifier'], row['ip']), axis=1)\n",
      "del installations['repository_identifier']\n",
      "del installations['ip']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.merge(installations, how=\"left\", left_index=True, right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record_dates = record_dates.reset_index().set_index('record_id')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# archive_settings cleanup to make Alex happy\n",
      "try: \n",
      "    del archive_settings\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    del lastIndexedDate\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    del recordCount\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    del countries\n",
      "except:\n",
      "    pass    \n",
      "\n",
      "try:\n",
      "    del installations\n",
      "except:\n",
      "    pass    \n",
      "\n",
      "cleanedup = gc.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "year_regex = re.compile('.*(?:[^\\d\\-]|^)((?:1\\d|20)\\d{2})(?:\\)|\\;|$).*')\n",
      "\n",
      "# pull the year out of the ( ) if it is present\n",
      "def find_best_year(y, s):\n",
      "    try:\n",
      "        y = int(y)\n",
      "        if not (y <= date.today().year and y > 1000):\n",
      "            y = None  # this means an invalid year in dcdate\n",
      "    except:\n",
      "        y = None\n",
      "\n",
      "    try: \n",
      "        r=year_regex.match(s)\n",
      "        year_in_source = int(r.group(1))\n",
      "        if year_in_source <= date.today().year and year_in_source > 1000:\n",
      "            y = year_in_source\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    return y\n",
      "# We're doing chained assignment somewhere here, but its not a problem, so turn off warning\n",
      "pd.options.mode.chained_assignment = None \n",
      "\n",
      "#\n",
      "# a simple attempt at fixing wrong publication years by pulling a year from the source\n",
      "#\n",
      "record_dates['year'] = record_dates.dcdate.astype(str).map(lambda x: x[0:4])\n",
      "record_dates['year'] = record_dates.apply(lambda row: find_best_year(row['year'], row['source']), axis=1)\n",
      "\n",
      "# cleanup the years column\n",
      "record_dates = record_dates[record_dates.year > 0] # remove things that still have no date\n",
      "\n",
      "# and cast column to int to remove .0 from the end when it gets turned to str later\n",
      "record_dates['year'] = record_dates.year.astype(int)\n",
      "\n",
      "\n",
      "# update dates that are likely to be in the islamic calendar\n",
      "record_dates['year'] = record_dates.year.map(lambda x: x if ((x <= date.today().year) and (x >= 1900 - 622)) else x + 622)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# grab only those journals that have at least x articles in that year\n",
      "last_year = date.today().year - 1\n",
      "\n",
      "def filter_by_num_articles(n):\n",
      "    grouped=record_dates.reset_index().groupby(['year', 'archive_id'])\n",
      "\n",
      "    filtered = grouped.filter(lambda x: len(x) >= n)\n",
      "\n",
      "    journals_data = filtered.ix[:,['archive_id', 'year']]\n",
      "    journals_data.to_csv('data/journals_with_%s_articles_any_year.csv' % n)\n",
      "\n",
      "    journals_data = filtered[filtered['year'] == last_year].ix[:,['archive_id','year']]\n",
      "    journals_data.to_csv('data/journals_with_%s_articles_%s.csv' % (n, last_year))\n",
      "\n",
      "    return filtered, journals_data\n",
      "\n",
      "def filter_by_num_articles_last_two_years(n): \n",
      "    archive_ids = record_dates[record_dates.year.between(last_year-2, last_year-1)].groupby('archive_id').filter(lambda x: len(x) >= n).archive_id.unique()\n",
      "    filtered = record_dates[record_dates.archive_id.isin(archive_ids)]\n",
      "    return filtered\n",
      "\n",
      "article_threshold = 10\n",
      "filtered, journals_data = filter_by_num_articles(article_threshold)\n",
      "# filtered = filter_by_num_articles_last_two_years(article_threshold)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "countdata = {}\n",
      "articles_per_journal = {}\n",
      "# record_dates = record_dates[record_dates.year >= 1990]\n",
      "if date.today().month > 4: \n",
      "    last_year = date.today().year - 1\n",
      "else:\n",
      "    last_year = date.today().year - 2\n",
      "    \n",
      "for year in map(int, sorted(record_dates.year.unique())):\n",
      "    if year < 1990 or year > last_year:\n",
      "        continue\n",
      "        \n",
      "    count_journals = len(filtered[filtered[\"year\"]==year].archive_id.unique())\n",
      "    count_articles = len(filtered[filtered[\"year\"]==year])\n",
      "    count_hosts = len(filtered[filtered[\"year\"]==year].install_id.unique())\n",
      "    avg_articles_per_journal = filtered[filtered['year']==year].groupby(['year', 'archive_id']).apply(len).mean()\n",
      "\n",
      "    # num_journals, num_articles, num_hosts\n",
      "    countdata[year] = [count_journals, count_articles, count_hosts, avg_articles_per_journal]\n",
      "\n",
      "    # get number of articles per journal per year\n",
      "    # count, mean, std, min, 25%, 50%, 75%, max\n",
      "    articles_per_journal[year] = record_dates[record_dates['year']==year].groupby('archive_id').size().describe().tolist()\n",
      "\n",
      "print \"Total number of journals in %s: %s\" % (year, count_journals)\n",
      "\n",
      "# write the journal/article/hosts count\n",
      "f = open('data/ojs_counts.csv', 'wb')\n",
      "csvWriter = csv.writer(f)\n",
      "csvWriter.writerow(['year', 'journals', 'articles', 'hosts', 'avgnumarts'])\n",
      "for year, data in countdata.iteritems():\n",
      "    csvWriter.writerow([year] + data)\n",
      "f.close()\n",
      "\n",
      "# write the articles per year counts\n",
      "f = open('data/articles_per_journal.csv', 'wb')\n",
      "csvWriter = csv.writer(f)\n",
      "csvWriter.writerow(['year', 'count', 'mean', 'std', 'min', 'p25', 'p50', 'p75', 'max'])\n",
      "for year, data in articles_per_journal.iteritems():\n",
      "    csvWriter.writerow([year] + data)\n",
      "f.close()\n",
      "\n",
      "# write the number of journals per country\n",
      "filtered_country = filtered.groupby(['year', 'country', 'region_id', 'region_name']).archive_id.unique().apply(len)\n",
      "filtered_country.to_csv('data/journals_per_country.csv', header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_journals = record_dates[['archive_id', 'journal_title', 'url', 'contact', 'lastIndexedDate', 'recordCount', 'country', 'region_name']].set_index('archive_id', drop=True).drop_duplicates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for y in countdata: \n",
      "    all_journals[y] = all_journals.index.isin(filtered[filtered[\"year\"] == y].archive_id)\n",
      "\n",
      "all_journals['any_year'] = all_journals.index.isin(filtered[filtered[\"year\"].between(min(countdata.keys()), max(countdata.keys()))].archive_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_journals.to_csv('data/all_journals.csv', header=True, encoding='utf8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Number of reachable installations: %s\" % len(record_dates.install_id.unique())\n",
      "print \"Number of countries: %s\" % len(record_dates.country.unique())\n",
      "print \"Number of world regions: %s\" % len(record_dates.region_name.unique())\n",
      "\n",
      "print \"Number of journals with > %s articles in previous two years: %s\" % (article_threshold, len(filtered.archive_id.unique()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Top by countries for %s: \" % last_year\n",
      "filtered_country = filtered[filtered.year==last_year].groupby(['country', 'region_id', 'region_name']).archive_id.unique().apply(len).reset_index()\n",
      "filtered_country.sort('archive_id', ascending=False).head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try: \n",
      "    import urllib2\n",
      "    request = urllib2.Request(\"http://api.worldbank.org/countries?per_page=500&format=json\" ,headers={'User-Agent' : \"PKP using urllib2\"})\n",
      "    response = urllib2.urlopen(request, timeout=30)\n",
      "    with open('data/worldbank.json', 'w') as f:\n",
      "        f.write(response.read())\n",
      "except:\n",
      "    pass\n",
      "    \n",
      "with open('data/worldbank.json') as f:\n",
      "    response = f.read()\n",
      "\n",
      "wb_country_data = json.loads(response)[1]\n",
      "wb_country_income_map = {w['id'].lower(): w['incomeLevel']['id'] for w in wb_country_data}\n",
      "filtered['incomeLevel'] = filtered.country.apply(lambda x: wb_country_income_map[x] if x in wb_country_income_map else None)\n",
      "income_level_names = {w['incomeLevel']['id']: w['incomeLevel']['value'] for w in wb_country_data}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "{w['region']['id'].lower(): w['region']['value'] for w in wb_country_data}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Regions sorted for %s: \" % last_year\n",
      "filtered_country = filtered[filtered.year==last_year].groupby(['region_id', 'region_name']).archive_id.unique().apply(len).reset_index()\n",
      "print filtered_country.sort('archive_id', ascending=False)\n",
      "print \n",
      "print \"North America & Europe: %s\" % filtered_country[filtered_country.region_id.isin(['NAC', 'ECS'])].archive_id.sum()\n",
      "print \"Total: %s\" % filtered_country.archive_id.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filtered_income = filtered.groupby('incomeLevel').archive_id.unique().apply(len).reset_index()\n",
      "filtered_income.set_index('incomeLevel', inplace=True)\n",
      "filtered_income.columns = ['journals']\n",
      "# filtered_income.reindex([u'LIC', u'LMC', u'UMC', u'HIC', u'INX']).plot(kind=\"bar\")\n",
      "print filtered_income.sort('journals', ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def by_income(l):\n",
      "    x = filtered_income.ix[l].sum()[0]\n",
      "    return (x, 100.0*x/filtered_income['journals'].sum())\n",
      "print \"Low income: %s (%.2f%%)\" % by_income(['LIC'])\n",
      "print \"Lower-middle income: %s (%.2f%%)\" % by_income(['LMC'])\n",
      "print \"Upper middle income %s (%.2f%%)\" % by_income(['UMC'])\n",
      "print \"High income %s (%.2f%%)\" % by_income(['HIC'])\n",
      "print \"Not Classified %s (%.2f%%)\" % by_income(['INX'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def by_income(l):\n",
      "    x = len(filtered_income[filtered_income.incomeLevel.isin(l)])\n",
      "    return (x, 100.0*x/len(filtered_income))\n",
      "\n",
      "filtered_income = filtered[filtered.year == 2015]\n",
      "print \"Low income: %s (%.1f%%)\" % by_income(['LIC'])\n",
      "print \"Lower-middle income: %s (%.1f%%)\" % by_income(['LMC'])\n",
      "print \"Upper middle income %s (%.1f%%)\" % by_income(['UMC'])\n",
      "print \"High income %s (%.1f%%)\" % by_income(['HIC'])\n",
      "print \"Not Classified %s (%.2f%%)\" % by_income(['INX'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = filtered_income.copy()\n",
      "# df['incomeLevel'] = df.incomeLevel.map(lambda x: x if x != 'NOC' and x != 'OEC' else 'HIC')\n",
      "df = df.groupby(['incomeLevel', 'archive_id']).apply(len) # this is 2015, see above\n",
      "df = df.reset_index().groupby('incomeLevel').mean()\n",
      "df[0].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Number of articles since 1990: %s\" % sum([x[1] for x in countdata.itervalues()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = record_dates.groupby(['year', 'archive_id']).apply(len).reset_index()\n",
      "y = 2010\n",
      "print \"On average %.2f articles per year since %s\" % (df[df.year >= y][0].mean(), y)\n",
      "print \"On average %.2f articles per year before %s\" % (df[df.year < y][0].mean(), y)\n",
      "\n",
      "df = df[(df.year >= 1990) & (df.year<=last_year)].groupby('year')[0].mean()\n",
      "# print\n",
      "# print df\n",
      "# df.plot(kind=\"bar\", title=\"Average Number of articles per journal\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# archive_settings[archive_settings.setting_name .head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# df['domain'] = df.url.map(lambda x: urlparse(x).netloc if str(x) > 4 and str(x)[0:4] == 'http' else None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# df_acuks = df[df.domain.str.endswith('ac.uk').fillna(False)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# harvesterUrl = archive_settings[archive_settings.setting_name == 'harvesterUrl'][['setting_value']]\n",
      "# harvesterUrl.columns=['url']\n",
      "# harvesterUrl['url'] = harvesterUrl.url.apply(find_journal_url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# all_journals.ix[3138]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# df_acuks[['journal_title', 'url', 'recordCount']].sort('recordCount', ascending=False).to_excel('data/acuks_for_2015.xls')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}